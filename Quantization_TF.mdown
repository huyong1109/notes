#Model Compression 


When modern neural networks were being developed, the biggest challenge was getting them to work at all!

##Pruning
 
##SqueezeNet


##Quantization

Basis: Shrink file sizes by storing the min and max for each layer, and then compressing each float value to an eight-bit integer representing the closest real number in a linear set of 256 within the range.


Step:

(1) Train
train your own model;

(2) Create nodes to freeze trained weights/biases

    def _freeze(self):
        regex = re.compile('^[^:]*')
        with tf.name_scope('assign_ops'):
           for tvar in tf.trainable_variables():
               tf.assign(tvar, tvar.eval(),
                         name = re.match(regex, tvar.name).group(0))

(3) Save graph

    with open(outfile, 'wb') as f:
        f.write(sesh.graph_def.SerializeToString())

(4) (Later) restore graph and get handles for desired nodes

    with open(graph_def, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    self.x, self.dropout, self.predictions = tf.import_graph_def(
        graph_def, return_elements = ['x:0', 'dropout:0', 'predictions:0'],
        name = '')
  
(5) Re-assign stored values

    with tf.Session(config = config) as sesh:
      assign_ops = [op for op in tf.Graph.get_operations(sesh.graph)
                            if 'assign_ops' in op.name]
      sesh.run(assign_ops)